{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CaPImYlymRVZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "import _pickle as cp\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot\n",
    "from astropy.stats import median_absolute_deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNZ1OdbLk9zL"
   },
   "source": [
    "# Loading Dataframe & splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "o931CjbPmPX_",
    "outputId": "3c5c1b61-08c2-42e3-d70b-20cf591ce168"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>DayOfWeek_1</th>\n",
       "      <th>DayOfWeek_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Mar</th>\n",
       "      <th>Apr</th>\n",
       "      <th>May</th>\n",
       "      <th>Jun</th>\n",
       "      <th>Jul</th>\n",
       "      <th>Aug</th>\n",
       "      <th>Sep</th>\n",
       "      <th>Oct</th>\n",
       "      <th>Nov</th>\n",
       "      <th>Dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.130240e+16</td>\n",
       "      <td>5263</td>\n",
       "      <td>555</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8.130240e+16</td>\n",
       "      <td>6064</td>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8.130240e+16</td>\n",
       "      <td>8314</td>\n",
       "      <td>821</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8.130240e+16</td>\n",
       "      <td>13995</td>\n",
       "      <td>1498</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8.130240e+16</td>\n",
       "      <td>4822</td>\n",
       "      <td>559</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store          Date  Sales  Customers  Open  Promo  StateHoliday  \\\n",
       "0      1  8.130240e+16   5263        555     1      1             0   \n",
       "1      2  8.130240e+16   6064        625     1      1             0   \n",
       "2      3  8.130240e+16   8314        821     1      1             0   \n",
       "3      4  8.130240e+16  13995       1498     1      1             0   \n",
       "4      5  8.130240e+16   4822        559     1      1             0   \n",
       "\n",
       "   SchoolHoliday  DayOfWeek_1  DayOfWeek_2  ...  Mar  Apr  May  Jun  Jul  Aug  \\\n",
       "0              1            0            0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1              1            0            0  ...  0.0  1.0  0.0  0.0  1.0  0.0   \n",
       "2              1            0            0  ...  0.0  1.0  0.0  0.0  1.0  0.0   \n",
       "3              1            0            0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4              1            0            0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   Sep  Oct  Nov  Dec  \n",
       "0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  1.0  0.0  0.0  \n",
       "2  0.0  1.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = pd.read_csv('combined.csv', index_col='Unnamed: 0')\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z1sOwuChmvLD"
   },
   "outputs": [],
   "source": [
    "labels = combined['Sales'].to_numpy().reshape(-1, 1)\n",
    "features = combined.drop(columns=['Sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-A0OJT2cxG6",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IGTD codebase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "9wfS9VBSc2t3"
   },
   "outputs": [],
   "source": [
    "# @title select_features_by_variation\n",
    "def select_features_by_variation(data, variation_measure='var', threshold=None, num=None, draw_histogram=False,\n",
    "                                 bins=100, log=False):\n",
    "    '''\n",
    "    This function evaluates the variations of individual features and returns the indices of features with large\n",
    "    variations. Missing values are ignored in evaluating variation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: numpy array or pandas data frame of numeric values, with a shape of [n_samples, n_features].\n",
    "    variation_metric: string indicating the metric used for evaluating feature variation. 'var' indicates variance;\n",
    "        'std' indicates standard deviation; 'mad' indicates median absolute deviation. Default is 'var'.\n",
    "    threshold: float. Features with a variation larger than threshold will be selected. Default is None.\n",
    "    num: positive integer. It is the number of features to be selected based on variation.\n",
    "        The number of selected features will be the smaller of num and the total number of\n",
    "        features with non-missing variations. Default is None. threshold and portion can not take values\n",
    "        and be used simultaneously.\n",
    "    draw_histogram: boolean, whether to draw a histogram of feature variations. Default is False.\n",
    "    bins: positive integer, the number of bins in the histogram. Default is the smaller of 50 and the number of\n",
    "        features with non-missing variations.\n",
    "    log: boolean, indicating whether the histogram should be drawn on log scale.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    indices: 1-D numpy array containing the indices of selected features. If both threshold and\n",
    "        portion are None, indices will be an empty array.\n",
    "    '''\n",
    "\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.values\n",
    "    elif not isinstance(data, np.ndarray):\n",
    "        print('Input data must be a numpy array or pandas data frame')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if variation_measure == 'std':\n",
    "        v_all = np.nanstd(a=data, axis=0)\n",
    "    elif variation_measure == 'mad':\n",
    "        v_all = median_absolute_deviation(data=data, axis=0, ignore_nan=True)\n",
    "    else:\n",
    "        v_all = np.nanvar(a=data, axis=0)\n",
    "\n",
    "    indices = np.where(np.invert(np.isnan(v_all)))[0]\n",
    "    v = v_all[indices]\n",
    "\n",
    "    if draw_histogram:\n",
    "        if len(v) < 50:\n",
    "            print('There must be at least 50 features with variation measures to draw a histogram')\n",
    "        else:\n",
    "            bins = int(min(bins, len(v)))\n",
    "            _ = plt.hist(v, bins=bins, log=log)\n",
    "            plt.show()\n",
    "\n",
    "    if threshold is None and num is None:\n",
    "        return np.array([])\n",
    "    elif threshold is not None and num is not None:\n",
    "        print('threshold and portion can not be used simultaneously. Only one of them can take a real value')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if threshold is not None:\n",
    "        indices = indices[np.where(v > threshold)[0]]\n",
    "    else:\n",
    "        n_f = int(min(num, len(v)))\n",
    "        indices = indices[np.argsort(-v)[:n_f]]\n",
    "\n",
    "    indices = np.sort(indices)\n",
    "\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "zdEOs2Zgc50N"
   },
   "outputs": [],
   "source": [
    "# @title min_max_transform\n",
    "def min_max_transform(data):\n",
    "    '''\n",
    "    This function does a linear transformation of each feature, so that the minimum and maximum values of a\n",
    "    feature are 0 and 1, respectively.\n",
    "\n",
    "    Input:\n",
    "    data: an input data array with a size of [n_sample, n_feature]\n",
    "    Return:\n",
    "    norm_data: the data array after transformation\n",
    "    '''\n",
    "\n",
    "    norm_data = np.empty(data.shape)\n",
    "    norm_data.fill(np.nan)\n",
    "    for i in range(data.shape[1]):\n",
    "        v = data[:, i].copy()\n",
    "        if np.max(v) == np.min(v):\n",
    "            norm_data[:, i] = 0\n",
    "        else:\n",
    "            v = (v - np.min(v)) / (np.max(v) - np.min(v))\n",
    "            norm_data[:, i] = v\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "oc8xOoFRc8WP"
   },
   "outputs": [],
   "source": [
    "# @title generate_feature_distance_ranking\n",
    "def generate_feature_distance_ranking(data, method='Pearson'):\n",
    "    '''\n",
    "    This function generates ranking of distances/dissimilarities between features for tabular data.\n",
    "\n",
    "    Input:\n",
    "    data: input data, n_sample by n_feature\n",
    "    method: 'Euclidean' calculates similarity between features based on Euclidean distance;\n",
    "        'Pearson' uses Pearson correlation coefficient to evaluate similarity between features;\n",
    "        'Spearman' uses Spearman correlation coefficient to evaluate similarity between features;\n",
    "        'set' uses Jaccard index to evaluate similarity between features that are binary variables.\n",
    "\n",
    "    Return:\n",
    "    ranking: symmetric ranking matrix based on dissimilarity\n",
    "    corr: matrix of distances between features\n",
    "    '''\n",
    "\n",
    "    num = data.shape[1]\n",
    "    if method == 'Pearson':\n",
    "        corr = np.corrcoef(np.transpose(data))\n",
    "    elif method == 'Spearman':\n",
    "        corr = spearmanr(data).correlation\n",
    "    elif method == 'Euclidean':\n",
    "        corr = squareform(pdist(np.transpose(data), metric='euclidean'))\n",
    "        corr = np.max(corr) - corr\n",
    "        corr = corr / np.max(corr)\n",
    "    elif method == 'set':  # This is the new set operation to calculate similarity. It does not tolerate all-zero features.\n",
    "        corr1 = np.dot(np.transpose(data), data)\n",
    "        corr2 = data.shape[0] - np.dot(np.transpose(1 - data), 1 - data)\n",
    "        corr = corr1 / corr2\n",
    "\n",
    "    corr = 1 - corr\n",
    "    corr = np.around(a=corr, decimals=10)\n",
    "\n",
    "    tril_id = np.tril_indices(num, k=-1)\n",
    "    rank = rankdata(corr[tril_id])\n",
    "    ranking = np.zeros((num, num))\n",
    "    ranking[tril_id] = rank\n",
    "    ranking = ranking + np.transpose(ranking)\n",
    "\n",
    "    return ranking, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "QaPTxeZ0c-rt"
   },
   "outputs": [],
   "source": [
    "# @title generate_matrix_distance_ranking\n",
    "def generate_matrix_distance_ranking(num_r, num_c, method='Euclidean', num=None):\n",
    "    '''\n",
    "    This function calculates the ranking of distances between all pairs of entries in a matrix of size num_r by num_c.\n",
    "\n",
    "    Input:\n",
    "    num_r: number of rows in the matrix\n",
    "    num_c: number of columns in the matrix\n",
    "    method: method used to calculate distance. Can be 'Euclidean' or 'Manhattan'.\n",
    "    num: number of real features. If None, num = num_r * num_c. If num < num_r * num_c, num_r * num_c - num\n",
    "        zeros will be padded to the image representation.\n",
    "\n",
    "    Return:\n",
    "    coordinate: a num-by-2 matrix giving the coordinates of elements in the matrix.\n",
    "    ranking: a num-by-num matrix giving the ranking of pair-wise distance.\n",
    "\n",
    "    '''\n",
    "\n",
    "    if num is None:\n",
    "        num = num_r * num_c\n",
    "\n",
    "    # generate the coordinates of elements in a matrix\n",
    "    for r in range(num_r):\n",
    "        if r == 0:\n",
    "            coordinate = np.transpose(np.vstack((np.zeros(num_c), range(num_c))))\n",
    "        else:\n",
    "            coordinate = np.vstack((coordinate, np.transpose(np.vstack((np.ones(num_c) * r, range(num_c))))))\n",
    "    coordinate = coordinate[:num, :]\n",
    "\n",
    "    # calculate the closeness of the elements\n",
    "    cord_dist = np.zeros((num, num))\n",
    "    if method == 'Euclidean':\n",
    "        for i in range(num):\n",
    "            cord_dist[i, :] = np.sqrt(np.square(coordinate[i, 0] * np.ones(num) - coordinate[:, 0]) +\n",
    "                                     np.square(coordinate[i, 1] * np.ones(num) - coordinate[:, 1]))\n",
    "    elif method == 'Manhattan':\n",
    "        for i in range(num):\n",
    "            cord_dist[i, :] = np.abs(coordinate[i, 0] * np.ones(num) - coordinate[:, 0]) + \\\n",
    "                             np.abs(coordinate[i, 1] * np.ones(num) - coordinate[:, 1])\n",
    "\n",
    "    # generate the ranking based on distance\n",
    "    tril_id = np.tril_indices(num, k=-1)\n",
    "    rank = rankdata(cord_dist[tril_id])\n",
    "    ranking = np.zeros((num, num))\n",
    "    ranking[tril_id] = rank\n",
    "    ranking = ranking + np.transpose(ranking)\n",
    "\n",
    "    coordinate = np.int64(coordinate)\n",
    "    return (coordinate[:, 0], coordinate[:, 1]), ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "u64T_ppNdBcq"
   },
   "outputs": [],
   "source": [
    "# @title IGTD_absolute_error\n",
    "def IGTD_absolute_error(source, target, max_step=1000, switch_t=0, val_step=50, min_gain=0.00001, random_state=1,\n",
    "                        save_folder=None, file_name=''):\n",
    "    '''\n",
    "    This function switches the order of rows (columns) in the source ranking matrix to make it similar to the target\n",
    "    ranking matrix. In each step, the algorithm randomly picks a row that has not been switched with others for\n",
    "    the longest time and checks all possible switch of this row, and selects the switch that reduces the\n",
    "    dissimilarity most. Dissimilarity (i.e. the error) is the summation of absolute difference of\n",
    "    lower triangular elements between the rearranged source ranking matrix and the target ranking matrix.\n",
    "\n",
    "    Input:\n",
    "    source: a symmetric ranking matrix with zero diagonal elements.\n",
    "    target: a symmetric ranking matrix with zero diagonal elements. 'source' and 'target' should have the same size.\n",
    "    max_step: the maximum steps that the algorithm should run if never converges.\n",
    "    switch_t: the threshold to determine whether feature switching should happen\n",
    "    val_step: number of steps for checking gain on the objective function to determine convergence\n",
    "    min_gain: if the objective function is not improved more than 'min_gain' in 'val_step' steps,\n",
    "        the algorithm terminates.\n",
    "    random_state: for setting random seed.\n",
    "    save_folder: a path to save the picture of source ranking matrix in the optimization process.\n",
    "    file_name: a string as part of the file names for saving results\n",
    "\n",
    "    Return:\n",
    "    index_record: indices to rearrange the rows(columns) in source obtained the optimization process\n",
    "    err_record: error obtained in the optimization process\n",
    "    run_time: the time at which each step is completed in the optimization process\n",
    "    '''\n",
    "\n",
    "    np.random.RandomState(seed=random_state)\n",
    "    if os.path.exists(save_folder):\n",
    "        shutil.rmtree(save_folder)\n",
    "    os.mkdir(save_folder)\n",
    "\n",
    "    source = source.copy()\n",
    "    num = source.shape[0]\n",
    "    tril_id = np.tril_indices(num, k=-1)\n",
    "    index = np.array(range(num))\n",
    "    index_record = np.empty((max_step + 1, num))\n",
    "    index_record.fill(np.nan)\n",
    "    index_record[0, :] = index.copy()\n",
    "\n",
    "    # calculate the error associated with each row\n",
    "    err_v = np.empty(num)\n",
    "    err_v.fill(np.nan)\n",
    "    for i in range(num):\n",
    "        err_v[i] = np.sum(np.abs(source[i, 0:i] - target[i, 0:i])) + \\\n",
    "                   np.sum(np.abs(source[(i + 1):, i] - target[(i + 1):, i]))\n",
    "\n",
    "    step_record = -np.ones(num)\n",
    "    err_record = [np.sum(abs(source[tril_id] - target[tril_id]))]\n",
    "    pre_err = err_record[0]\n",
    "    t1 = time.time()\n",
    "    run_time = [0]\n",
    "\n",
    "    for s in range(max_step):\n",
    "        delta = - np.ones(num) * np.inf\n",
    "\n",
    "        # randomly pick a row that has not been considered for the longest time\n",
    "        idr = np.where(step_record == np.min(step_record))[0]\n",
    "        ii = idr[np.random.permutation(len(idr))[0]]\n",
    "\n",
    "        for jj in range(num):\n",
    "            if jj == ii:\n",
    "                continue\n",
    "\n",
    "            if ii < jj:\n",
    "                i = ii\n",
    "                j = jj\n",
    "            else:\n",
    "                i = jj\n",
    "                j = ii\n",
    "\n",
    "            err_ori = err_v[i] + err_v[j] - np.abs(source[j, i] - target[j, i])\n",
    "\n",
    "            err_i = np.sum(np.abs(source[j, :i] - target[i, :i])) + \\\n",
    "                    np.sum(np.abs(source[(i + 1):j, j] - target[(i + 1):j, i])) + \\\n",
    "                    np.sum(np.abs(source[(j + 1):, j] - target[(j + 1):, i])) + np.abs(source[i, j] - target[j, i])\n",
    "            err_j = np.sum(np.abs(source[i, :i] - target[j, :i])) + \\\n",
    "                    np.sum(np.abs(source[i, (i + 1):j] - target[j, (i + 1):j])) + \\\n",
    "                    np.sum(np.abs(source[(j + 1):, i] - target[(j + 1):, j])) + np.abs(source[i, j] - target[j, i])\n",
    "            err_test = err_i + err_j - np.abs(source[i, j] - target[j, i])\n",
    "\n",
    "            delta[jj] = err_ori - err_test\n",
    "\n",
    "        delta_norm = delta / pre_err\n",
    "        id = np.where(delta_norm >= switch_t)[0]\n",
    "        if len(id) > 0:\n",
    "            jj = np.argmax(delta)\n",
    "\n",
    "            # Update the error associated with each row\n",
    "            if ii < jj:\n",
    "                i = ii\n",
    "                j = jj\n",
    "            else:\n",
    "                i = jj\n",
    "                j = ii\n",
    "            for k in range(num):\n",
    "                if k < i:\n",
    "                    err_v[k] = err_v[k] - np.abs(source[i, k] - target[i, k]) - np.abs(source[j, k] - target[j, k]) + \\\n",
    "                               np.abs(source[j, k] - target[i, k]) + np.abs(source[i, k] - target[j, k])\n",
    "                elif k == i:\n",
    "                    err_v[k] = np.sum(np.abs(source[j, :i] - target[i, :i])) + \\\n",
    "                    np.sum(np.abs(source[(i + 1):j, j] - target[(i + 1):j, i])) + \\\n",
    "                    np.sum(np.abs(source[(j + 1):, j] - target[(j + 1):, i])) + np.abs(source[i, j] - target[j, i])\n",
    "                elif k < j:\n",
    "                    err_v[k] = err_v[k] - np.abs(source[k, i] - target[k, i]) - np.abs(source[j, k] - target[j, k]) + \\\n",
    "                               np.abs(source[k, j] - target[k, i]) + np.abs(source[i, k] - target[j, k])\n",
    "                elif k == j:\n",
    "                    err_v[k] = np.sum(np.abs(source[i, :i] - target[j, :i])) + \\\n",
    "                    np.sum(np.abs(source[i, (i + 1):j] - target[j, (i + 1):j])) + \\\n",
    "                    np.sum(np.abs(source[(j + 1):, i] - target[(j + 1):, j])) + np.abs(source[i, j] - target[j, i])\n",
    "                else:\n",
    "                    err_v[k] = err_v[k] - np.abs(source[k, i] - target[k, i]) - np.abs(source[k, j] - target[k, j]) + \\\n",
    "                               np.abs(source[k, j] - target[k, i]) + np.abs(source[k, i] - target[k, j])\n",
    "\n",
    "            # switch rows i and j\n",
    "            ii_v = source[ii, :].copy()\n",
    "            jj_v = source[jj, :].copy()\n",
    "            source[ii, :] = jj_v\n",
    "            source[jj, :] = ii_v\n",
    "            ii_v = source[:, ii].copy()\n",
    "            jj_v = source[:, jj].copy()\n",
    "            source[:, ii] = jj_v\n",
    "            source[:, jj] = ii_v\n",
    "            err = pre_err - delta[jj]\n",
    "\n",
    "            # update rearrange index\n",
    "            t = index[ii]\n",
    "            index[ii] = index[jj]\n",
    "            index[jj] = t\n",
    "\n",
    "            # update step record\n",
    "            step_record[ii] = s\n",
    "            step_record[jj] = s\n",
    "        else:\n",
    "            # error is not changed due to no switch\n",
    "            err = pre_err\n",
    "\n",
    "            # update step record\n",
    "            step_record[ii] = s\n",
    "\n",
    "        err_record.append(err)\n",
    "        print('Step ' + str(s) + ' err: ' + str(err))\n",
    "        index_record[s + 1, :] = index.copy()\n",
    "        run_time.append(time.time() - t1)\n",
    "\n",
    "        if s > val_step:\n",
    "            if np.sum((err_record[-val_step - 1] - np.array(err_record[(-val_step):])) / err_record[\n",
    "                -val_step - 1] >= min_gain) == 0:\n",
    "                break\n",
    "\n",
    "        pre_err = err\n",
    "\n",
    "    index_record = index_record[:len(err_record), :].astype(int)\n",
    "    if save_folder is not None:\n",
    "        pd.DataFrame(index_record).to_csv(save_folder + '/' + file_name + '_index.txt', header=False, index=False,\n",
    "            sep='\\t', lineterminator='\\r\\n')\n",
    "        pd.DataFrame(np.transpose(np.vstack((err_record, np.array(range(s + 2))))),\n",
    "            columns=['error', 'steps']).to_csv(save_folder + '/' + file_name + '_error_and_step.txt',\n",
    "            header=True, index=False, sep='\\t', lineterminator='\\r\\n')\n",
    "        pd.DataFrame(np.transpose(np.vstack((err_record, run_time))), columns=['error', 'run_time']).to_csv(\n",
    "            save_folder + '/' + file_name + '_error_and_time.txt', header=True, index=False, sep='\\t',\n",
    "            lineterminator='\\r\\n')\n",
    "\n",
    "    return index_record, err_record, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "1q0hIAkddEP4"
   },
   "outputs": [],
   "source": [
    "# @title IGTD_square_error\n",
    "def IGTD_square_error(source, target, max_step=1000, switch_t=0, val_step=50, min_gain=0.00001, random_state=1,\n",
    "                      save_folder=None, file_name=''):\n",
    "    '''\n",
    "    This function switches the order of rows (columns) in the source ranking matrix to make it similar to the target\n",
    "    ranking matrix. In each step, the algorithm randomly picks a row that has not been switched with others for\n",
    "    the longest time and checks all possible switch of this row, and selects the switch that reduces the\n",
    "    dissimilarity most. Dissimilarity (i.e. the error) is the summation of squared difference of\n",
    "    lower triangular elements between the rearranged source ranking matrix and the target ranking matrix.\n",
    "\n",
    "    Input:\n",
    "    source: a symmetric ranking matrix with zero diagonal elements.\n",
    "    target: a symmetric ranking matrix with zero diagonal elements. 'source' and 'target' should have the same size.\n",
    "    max_step: the maximum steps that the algorithm should run if never converges.\n",
    "    switch_t: the threshold to determine whether feature switching should happen\n",
    "    val_step: number of steps for checking gain on the objective function to determine convergence\n",
    "    min_gain: if the objective function is not improved more than 'min_gain' in 'val_step' steps,\n",
    "        the algorithm terminates.\n",
    "    random_state: for setting random seed.\n",
    "    save_folder: a path to save the picture of source ranking matrix in the optimization process.\n",
    "    file_name: a string as part of the file names for saving results\n",
    "\n",
    "    Return:\n",
    "    index_record: ordering index to rearrange the rows(columns) in 'source' in the optimization process\n",
    "    err_record: the error history in the optimization process\n",
    "    run_time: the time at which each step is finished in the optimization process\n",
    "    '''\n",
    "\n",
    "\n",
    "    np.random.RandomState(seed=random_state)\n",
    "    if os.path.exists(save_folder):\n",
    "        shutil.rmtree(save_folder)\n",
    "    os.mkdir(save_folder)\n",
    "\n",
    "    source = source.copy()\n",
    "    num = source.shape[0]\n",
    "    tril_id = np.tril_indices(num, k=-1)\n",
    "    index = np.array(range(num))\n",
    "    index_record = np.empty((max_step + 1, num))\n",
    "    index_record.fill(np.nan)\n",
    "    index_record[0, :] = index.copy()\n",
    "\n",
    "    # calculate the error associated with each row\n",
    "    err_v = np.empty(num)\n",
    "    err_v.fill(np.nan)\n",
    "    for i in range(num):\n",
    "        err_v[i] = np.sum(np.square(source[i, 0:i] - target[i, 0:i])) + \\\n",
    "                   np.sum(np.square(source[(i + 1):, i] - target[(i + 1):, i]))\n",
    "\n",
    "    step_record = -np.ones(num)\n",
    "    err_record = [np.sum(np.square(source[tril_id] - target[tril_id]))]\n",
    "    pre_err = err_record[0]\n",
    "    t1 = time.time()\n",
    "    run_time = [0]\n",
    "\n",
    "    for s in range(max_step):\n",
    "        delta = - np.ones(num) * np.inf\n",
    "\n",
    "        # randomly pick a row that has not been considered for the longest time\n",
    "        idr = np.where(step_record == np.min(step_record))[0]\n",
    "        ii = idr[np.random.permutation(len(idr))[0]]\n",
    "\n",
    "        for jj in range(num):\n",
    "            if jj == ii:\n",
    "                continue\n",
    "\n",
    "            if ii < jj:\n",
    "                i = ii\n",
    "                j = jj\n",
    "            else:\n",
    "                i = jj\n",
    "                j = ii\n",
    "\n",
    "            err_ori = err_v[i] + err_v[j] - np.square(source[j, i] - target[j, i])\n",
    "\n",
    "            err_i = np.sum(np.square(source[j, :i] - target[i, :i])) + \\\n",
    "                    np.sum(np.square(source[(i + 1):j, j] - target[(i + 1):j, i])) + \\\n",
    "                    np.sum(np.square(source[(j + 1):, j] - target[(j + 1):, i])) + np.square(source[i, j] - target[j, i])\n",
    "            err_j = np.sum(np.square(source[i, :i] - target[j, :i])) + \\\n",
    "                    np.sum(np.square(source[i, (i + 1):j] - target[j, (i + 1):j])) + \\\n",
    "                    np.sum(np.square(source[(j + 1):, i] - target[(j + 1):, j])) + np.square(source[i, j] - target[j, i])\n",
    "            err_test = err_i + err_j - np.square(source[i, j] - target[j, i])\n",
    "\n",
    "            delta[jj] = err_ori - err_test\n",
    "\n",
    "        delta_norm = delta / pre_err\n",
    "        id = np.where(delta_norm >= switch_t)[0]\n",
    "        if len(id) > 0:\n",
    "            jj = np.argmax(delta)\n",
    "\n",
    "            # Update the error associated with each row\n",
    "            if ii < jj:\n",
    "                i = ii\n",
    "                j = jj\n",
    "            else:\n",
    "                i = jj\n",
    "                j = ii\n",
    "            for k in range(num):\n",
    "                if k < i:\n",
    "                    err_v[k] = err_v[k] - np.square(source[i, k] - target[i, k]) - np.square(source[j, k] - target[j, k]) + \\\n",
    "                               np.square(source[j, k] - target[i, k]) + np.square(source[i, k] - target[j, k])\n",
    "                elif k == i:\n",
    "                    err_v[k] = np.sum(np.square(source[j, :i] - target[i, :i])) + \\\n",
    "                        np.sum(np.square(source[(i + 1):j, j] - target[(i + 1):j, i])) + \\\n",
    "                        np.sum(np.square(source[(j + 1):, j] - target[(j + 1):, i])) + np.square(source[i, j] - target[j, i])\n",
    "                elif k < j:\n",
    "                    err_v[k] = err_v[k] - np.square(source[k, i] - target[k, i]) - np.square(source[j, k] - target[j, k]) + \\\n",
    "                               np.square(source[k, j] - target[k, i]) + np.square(source[i, k] - target[j, k])\n",
    "                elif k == j:\n",
    "                    err_v[k] = np.sum(np.square(source[i, :i] - target[j, :i])) + \\\n",
    "                        np.sum(np.square(source[i, (i + 1):j] - target[j, (i + 1):j])) + \\\n",
    "                        np.sum(np.square(source[(j + 1):, i] - target[(j + 1):, j])) + np.square(source[i, j] - target[j, i])\n",
    "                else:\n",
    "                    err_v[k] = err_v[k] - np.square(source[k, i] - target[k, i]) - np.square(source[k, j] - target[k, j]) + \\\n",
    "                               np.square(source[k, j] - target[k, i]) + np.square(source[k, i] - target[k, j])\n",
    "\n",
    "            # switch rows i and j\n",
    "            ii_v = source[ii, :].copy()\n",
    "            jj_v = source[jj, :].copy()\n",
    "            source[ii, :] = jj_v\n",
    "            source[jj, :] = ii_v\n",
    "            ii_v = source[:, ii].copy()\n",
    "            jj_v = source[:, jj].copy()\n",
    "            source[:, ii] = jj_v\n",
    "            source[:, jj] = ii_v\n",
    "            err = pre_err - delta[jj]\n",
    "\n",
    "            # update rearrange index\n",
    "            t = index[ii]\n",
    "            index[ii] = index[jj]\n",
    "            index[jj] = t\n",
    "\n",
    "            # update step record\n",
    "            step_record[ii] = s\n",
    "            step_record[jj] = s\n",
    "        else:\n",
    "            # error is not changed due to no switch\n",
    "            err = pre_err\n",
    "\n",
    "            # update step record\n",
    "            step_record[ii] = s\n",
    "\n",
    "        err_record.append(err)\n",
    "        print('Step ' + str(s) + ' err: ' + str(err))\n",
    "        index_record[s + 1, :] = index.copy()\n",
    "        run_time.append(time.time() - t1)\n",
    "\n",
    "        if s > val_step:\n",
    "            if np.sum((err_record[-val_step - 1] - np.array(err_record[(-val_step):])) / err_record[\n",
    "                -val_step - 1] >= min_gain) == 0:\n",
    "                break\n",
    "\n",
    "        pre_err = err\n",
    "\n",
    "    index_record = index_record[:len(err_record), :].astype(int)\n",
    "    if save_folder is not None:\n",
    "        pd.DataFrame(index_record).to_csv(save_folder + '/' + file_name + '_index.txt', header=False, index=False,\n",
    "            sep='\\t', lineterminator='\\r\\n')\n",
    "        pd.DataFrame(np.transpose(np.vstack((err_record, np.array(range(s + 2))))),\n",
    "            columns=['error', 'steps']).to_csv(save_folder + '/' + file_name + '_error_and_step.txt',\n",
    "            header=True, index=False, sep='\\t', lineterminator='\\r\\n')\n",
    "        pd.DataFrame(np.transpose(np.vstack((err_record, run_time))), columns=['error', 'run_time']).to_csv(\n",
    "            save_folder + '/' + file_name + '_error_and_time.txt', header=True, index=False, sep='\\t',\n",
    "            lineterminator='\\r\\n')\n",
    "\n",
    "    return index_record, err_record, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "id": "tttYhMsEdGqL"
   },
   "outputs": [],
   "source": [
    "# @title IGTD\n",
    "def IGTD(source, target, err_measure='abs', max_step=1000, switch_t=0, val_step=50, min_gain=0.00001, random_state=1,\n",
    "         save_folder=None, file_name=''):\n",
    "    '''\n",
    "    This is just a wrapper function that wraps the two search functions using different error measures.\n",
    "    '''\n",
    "\n",
    "    if err_measure == 'abs':\n",
    "        index_record, err_record, run_time = IGTD_absolute_error(source=source,\n",
    "            target=target, max_step=max_step, switch_t=switch_t, val_step=val_step, min_gain=min_gain,\n",
    "            random_state=random_state, save_folder=save_folder, file_name=file_name)\n",
    "    if err_measure == 'squared':\n",
    "        index_record, err_record, run_time = IGTD_square_error(source=source,\n",
    "            target=target, max_step=max_step, switch_t=switch_t, val_step=val_step, min_gain=min_gain,\n",
    "            random_state=random_state, save_folder=save_folder, file_name=file_name)\n",
    "\n",
    "    return index_record, err_record, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SfViDToodIxS"
   },
   "outputs": [],
   "source": [
    "# @title generate_image_data\n",
    "def generate_image_data(data, index, num_row, num_column, coord, image_folder=None, file_name=''):\n",
    "    '''\n",
    "    This function generates the data in image format according to rearrangement indices. It saves the data\n",
    "    sample-by-sample in both txt files and image files\n",
    "\n",
    "    Input:\n",
    "    data: original tabular data, 2D array or data frame, n_samples by n_features\n",
    "    index: indices of features obtained through optimization, according to which the features can be\n",
    "        arranged into a num_r by num_c image.\n",
    "    num_row: number of rows in image\n",
    "    num_column: number of columns in image\n",
    "    coord: coordinates of features in the image/matrix\n",
    "    image_folder: directory to save the image and txt data files. If none, no data file is saved\n",
    "    file_name: a string as a part of the file names to save data\n",
    "\n",
    "    Return:\n",
    "    image_data: the generated data, a 3D numpy array. The third dimension is across samples. The range of values\n",
    "        is [0, 255]. Small values actually indicate high values in the original data.\n",
    "    samples: the names of indices of the samples\n",
    "    '''\n",
    "\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        samples = data.index.map(np.str)\n",
    "        data = data.values\n",
    "    else:\n",
    "        samples = [str(i) for i in range(data.shape[0])]\n",
    "\n",
    "    if os.path.exists(image_folder):\n",
    "        shutil.rmtree(image_folder)\n",
    "    os.mkdir(image_folder)\n",
    "\n",
    "    data_2 = data.copy()\n",
    "    data_2 = data_2[:, index]\n",
    "    max_v = np.max(data_2)\n",
    "    min_v = np.min(data_2)\n",
    "    data_2 = 255 - (data_2 - min_v) / (max_v - min_v) * 255 # Black color in heatmap indicates high value\n",
    "\n",
    "    image_data = np.empty((num_row, num_column, data_2.shape[0]))\n",
    "    image_data.fill(np.nan)\n",
    "    for i in range(data_2.shape[0]):\n",
    "        data_i = np.empty((num_row, num_column))\n",
    "        data_i.fill(np.nan)\n",
    "        data_i[coord] = data_2[i, :]\n",
    "\n",
    "        # find nan in data_i and change them to 255\n",
    "        idd = np.where(np.isnan(data_i))\n",
    "        data_i[idd] = 255\n",
    "\n",
    "        image_data[:, :, i] = data_i\n",
    "        image_data[:, :, i] = 255 - image_data[:, :, i] # High values in the array format of image data correspond\n",
    "                                                        # to high values in tabular data\n",
    "        # Commented out by Moi, dont need it\n",
    "        # if image_folder is not None:\n",
    "        #     fig = plt.figure()\n",
    "        #     plt.imshow(data_i, cmap='gray', vmin=0, vmax=255)\n",
    "        #     plt.axis('scaled')\n",
    "        #     plt.savefig(fname=image_folder + '/' + file_name + '_' + samples[i] + '_image.png', bbox_inches='tight',\n",
    "        #                 pad_inches=0)\n",
    "        #     plt.close(fig)\n",
    "\n",
    "        #     pd.DataFrame(image_data[:, :, i], index=None, columns=None).to_csv(image_folder + '/' + file_name + '_'\n",
    "        #         + samples[i] + '_data.txt', header=None, index=None, sep='\\t', line_terminator='\\r\\n')\n",
    "\n",
    "    return image_data, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "id": "YOKy5hGMdLTo"
   },
   "outputs": [],
   "source": [
    "# @title table_to_image\n",
    "def table_to_image(norm_d, scale, fea_dist_method, image_dist_method, save_image_size, max_step, val_step, normDir,\n",
    "                   error, switch_t=0, min_gain=0.00001):\n",
    "    '''\n",
    "    This function converts tabular data into images using the IGTD algorithm.\n",
    "\n",
    "    Input:\n",
    "    norm_d: a 2D array or data frame, which is the tabular data. Its size is n_samples by n_features\n",
    "    scale: a list of two positive integers. It includes the numbers of pixel rows and columns in the image\n",
    "        representation. The total number of pixels should not be smaller than the number of features,\n",
    "        i.e. scale[0] * scale[1] >= n_features.\n",
    "    fea_dist_method: a string indicating the method used for calculating the pairwise distances between features,\n",
    "        for which there are three options.\n",
    "        'Pearson' uses the Pearson correlation coefficient to evaluate the similarity between features.\n",
    "        'Spearman' uses the Spearman correlation coefficient to evaluate the similarity between features.\n",
    "        'set' uses the Jaccard index to evaluate the similarity between features that are binary variables.\n",
    "    image_dist_method: a string indicating the method used for calculating the distances between pixels in image.\n",
    "        It can be either 'Euclidean' or 'Manhattan'.\n",
    "    save_image_size: size of images (in inches) for saving visual results.\n",
    "    max_step: the maximum number of iterations that the IGTD algorithm will run if never converges.\n",
    "    val_step: the number of iterations for determining algorithm convergence. If the error reduction rate is smaller than\n",
    "        min_gain for val_step iterations, the algorithm converges.\n",
    "    normDir: a string indicating the directory to save result files.\n",
    "    error: a string indicating the function to evaluate the difference between feature distance ranking and pixel\n",
    "        distance ranking. 'abs' indicates the absolute function. 'squared' indicates the square function.\n",
    "    switch_t: the threshold on error change rate. Error change rate is\n",
    "        (error before feature swapping - error after feature swapping) / error before feature swapping.\n",
    "        In each iteration, if the largest error change rate resulted from all possible feature swappings\n",
    "        is not smaller than switch_t, the feature swapping resulting in the largest error change rate will\n",
    "        be performed. If switch_t >= 0, the IGTD algorithm monotonically reduces the error during optimization.\n",
    "    min_gain: if the error reduction rate is not larger than min_gain for val_step iterations, the algorithm converges.\n",
    "\n",
    "    Return:\n",
    "    This function does not return any variable, but saves multiple result files, which are the following\n",
    "    1.  Results.pkl stores the original tabular data, the generated image data, and the names of samples. The generated\n",
    "        image data is a 3D numpy array. Its size is [number of pixel rows in image, number of pixel columns in image,\n",
    "        number of samples]. The range of values is [0, 255]. Small values in the array actually correspond to high\n",
    "        values in the tabular data.\n",
    "    2.  Results_Auxiliary.pkl stores the ranking matrix of pairwise feature distances before optimization,\n",
    "        the ranking matrix of pairwise pixel distances, the coordinates of pixels when concatenating pixels\n",
    "        row by row from image to form the pixel distance ranking matrix, error in each iteration,\n",
    "        and time (in seconds) when completing each iteration.\n",
    "    3.  original_feature_ranking.png shows the feature distance ranking matrix before optimization.\n",
    "    4.  image_ranking.png shows the pixel distance ranking matrix.\n",
    "    5.  error_and_runtime.png shows the change of error vs. time during the optimization process.\n",
    "    6.  error_and_iteration.png shows the change of error vs. iteration during the optimization process.\n",
    "    7.  optimized_feature_ranking.png shows the feature distance ranking matrix after optimization.\n",
    "    8.  data folder includes two image data files for each sample. The txt file is the image data in matrix format,\n",
    "        in which high values correspond to high values of features in tabular data. The png file shows the\n",
    "        visualization of image data, in which black and white correspond to high and low values of features in\n",
    "        tabular data, respectively.\n",
    "    '''\n",
    "\n",
    "    if os.path.exists(normDir):\n",
    "        shutil.rmtree(normDir)\n",
    "    os.mkdir(normDir)\n",
    "\n",
    "    ranking_feature, corr = generate_feature_distance_ranking(data=norm_d, method=fea_dist_method)\n",
    "    fig = plt.figure(figsize=(save_image_size, save_image_size))\n",
    "    plt.imshow(np.max(ranking_feature) - ranking_feature, cmap='gray', interpolation='nearest')\n",
    "    plt.savefig(fname=normDir + '/original_feature_ranking.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "    coordinate, ranking_image = generate_matrix_distance_ranking(num_r=scale[0], num_c=scale[1],\n",
    "                                                                 method=image_dist_method, num=norm_d.shape[1])\n",
    "    fig = plt.figure(figsize=(save_image_size, save_image_size))\n",
    "    plt.imshow(np.max(ranking_image) - ranking_image, cmap='gray', interpolation='nearest')\n",
    "    plt.savefig(fname=normDir + '/image_ranking.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "    index, err, time = IGTD(source=ranking_feature, target=ranking_image,\n",
    "        err_measure=error, max_step=max_step, switch_t=switch_t, val_step=val_step, min_gain=min_gain, random_state=1,\n",
    "        save_folder=normDir + '/' + error, file_name='')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(time, err)\n",
    "    plt.savefig(fname=normDir + '/error_and_runtime.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(len(err)), err)\n",
    "    plt.savefig(fname=normDir + '/error_and_iteration.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "    min_id = np.argmin(err)\n",
    "    ranking_feature_random = ranking_feature[index[min_id, :], :]\n",
    "    ranking_feature_random = ranking_feature_random[:, index[min_id, :]]\n",
    "\n",
    "    fig = plt.figure(figsize=(save_image_size, save_image_size))\n",
    "    plt.imshow(np.max(ranking_feature_random) - ranking_feature_random, cmap='gray',\n",
    "               interpolation='nearest')\n",
    "    plt.savefig(fname=normDir + '/optimized_feature_ranking.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "    data, samples = generate_image_data(data=norm_d, index=index[min_id, :], num_row=scale[0], num_column=scale[1],\n",
    "        coord=coordinate, image_folder=normDir + '/data', file_name='')\n",
    "\n",
    "    return data, samples\n",
    "\n",
    "    # output = open(normDir + '/Results.pkl', 'wb')\n",
    "    # cp.dump(norm_d, output)\n",
    "    # cp.dump(data, output)\n",
    "    # cp.dump(samples, output)\n",
    "    # output.close()\n",
    "\n",
    "    # output = open(normDir + '/Results_Auxiliary.pkl', 'wb')\n",
    "    # cp.dump(ranking_feature, output)\n",
    "    # cp.dump(ranking_image, output)\n",
    "    # cp.dump(coordinate, output)\n",
    "    # cp.dump(err, output)\n",
    "    # cp.dump(time, output)\n",
    "    # output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXG4wcqRn2qq"
   },
   "source": [
    "# IGTD Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qbbnZYmhn72B"
   },
   "outputs": [],
   "source": [
    "num_row = 5  # Number of pixel rows in image representation\n",
    "num_col = 5   # Number of pixel columns in image representation\n",
    "num = num_row * num_col # Number of features to be included for analysis, which is also the total number of pixels in image representation\n",
    "save_image_size = 3 # Size of pictures (in inches) saved during the execution of IGTD algorithm.\n",
    "max_step = 10000    # The maximum number of iterations to run the IGTD algorithm, if it does not converge.\n",
    "val_step = 300  # The number of iterations for determining algorithm convergence. If the error reduction rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0B08A2lpG9O",
    "outputId": "2d7745eb-45a7-4684-9403-2a99c8dd4d3a"
   },
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "id = select_features_by_variation(features, variation_measure='var', num=num)\n",
    "features = features.iloc[:, id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SDHWC7JpLNc",
    "outputId": "b783bd34-1c6c-43f3-b43e-b35ecacabbfc"
   },
   "outputs": [],
   "source": [
    "# Feature & Label Scaling\n",
    "l_scaler = MinMaxScaler()\n",
    "f_scaler = MinMaxScaler()\n",
    "\n",
    "labels_scaled = l_scaler.fit_transform(labels)\n",
    "\n",
    "features_scaled = f_scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTkMOfk7oW4R",
    "outputId": "476940f6-c242-40db-acb4-5e23ceac28be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 err: 26775.0\n",
      "Step 1 err: 26379.0\n",
      "Step 2 err: 25448.0\n",
      "Step 3 err: 24846.0\n",
      "Step 4 err: 24634.0\n",
      "Step 5 err: 24370.0\n",
      "Step 6 err: 23433.0\n",
      "Step 7 err: 22110.0\n",
      "Step 8 err: 22064.0\n",
      "Step 9 err: 21400.0\n",
      "Step 10 err: 20836.0\n",
      "Step 11 err: 20481.0\n",
      "Step 12 err: 20481.0\n",
      "Step 13 err: 20030.0\n",
      "Step 14 err: 19329.0\n",
      "Step 15 err: 19269.0\n",
      "Step 16 err: 18883.0\n",
      "Step 17 err: 18883.0\n",
      "Step 18 err: 18718.0\n",
      "Step 19 err: 18718.0\n",
      "Step 20 err: 18283.0\n",
      "Step 21 err: 18140.0\n",
      "Step 22 err: 18140.0\n",
      "Step 23 err: 18140.0\n",
      "Step 24 err: 18140.0\n",
      "Step 25 err: 18130.0\n",
      "Step 26 err: 18130.0\n",
      "Step 27 err: 18130.0\n",
      "Step 28 err: 18130.0\n",
      "Step 29 err: 18130.0\n",
      "Step 30 err: 18130.0\n",
      "Step 31 err: 18130.0\n",
      "Step 32 err: 18130.0\n",
      "Step 33 err: 17944.0\n",
      "Step 34 err: 17944.0\n",
      "Step 35 err: 17944.0\n",
      "Step 36 err: 17944.0\n",
      "Step 37 err: 17944.0\n",
      "Step 38 err: 17944.0\n",
      "Step 39 err: 17944.0\n",
      "Step 40 err: 17944.0\n",
      "Step 41 err: 17944.0\n",
      "Step 42 err: 17944.0\n",
      "Step 43 err: 17944.0\n",
      "Step 44 err: 17944.0\n",
      "Step 45 err: 17942.0\n",
      "Step 46 err: 17942.0\n",
      "Step 47 err: 17942.0\n",
      "Step 48 err: 17942.0\n",
      "Step 49 err: 17942.0\n",
      "Step 50 err: 17942.0\n",
      "Step 51 err: 17942.0\n",
      "Step 52 err: 17942.0\n",
      "Step 53 err: 17942.0\n",
      "Step 54 err: 17942.0\n",
      "Step 55 err: 17942.0\n",
      "Step 56 err: 17942.0\n",
      "Step 57 err: 17942.0\n",
      "Step 58 err: 17942.0\n",
      "Step 59 err: 17942.0\n",
      "Step 60 err: 17942.0\n",
      "Step 61 err: 17942.0\n",
      "Step 62 err: 17942.0\n",
      "Step 63 err: 17942.0\n",
      "Step 64 err: 17942.0\n",
      "Step 65 err: 17942.0\n",
      "Step 66 err: 17942.0\n",
      "Step 67 err: 17942.0\n",
      "Step 68 err: 17942.0\n",
      "Step 69 err: 17942.0\n",
      "Step 70 err: 17942.0\n",
      "Step 71 err: 17942.0\n",
      "Step 72 err: 17942.0\n",
      "Step 73 err: 17942.0\n",
      "Step 74 err: 17942.0\n",
      "Step 75 err: 17942.0\n",
      "Step 76 err: 17942.0\n",
      "Step 77 err: 17942.0\n",
      "Step 78 err: 17942.0\n",
      "Step 79 err: 17942.0\n",
      "Step 80 err: 17942.0\n",
      "Step 81 err: 17942.0\n",
      "Step 82 err: 17942.0\n",
      "Step 83 err: 17942.0\n",
      "Step 84 err: 17942.0\n",
      "Step 85 err: 17942.0\n",
      "Step 86 err: 17942.0\n",
      "Step 87 err: 17942.0\n",
      "Step 88 err: 17942.0\n",
      "Step 89 err: 17942.0\n",
      "Step 90 err: 17942.0\n",
      "Step 91 err: 17942.0\n",
      "Step 92 err: 17942.0\n",
      "Step 93 err: 17942.0\n",
      "Step 94 err: 17942.0\n",
      "Step 95 err: 17942.0\n",
      "Step 96 err: 17942.0\n",
      "Step 97 err: 17942.0\n",
      "Step 98 err: 17942.0\n",
      "Step 99 err: 17942.0\n",
      "Step 100 err: 17942.0\n",
      "Step 101 err: 17942.0\n",
      "Step 102 err: 17942.0\n",
      "Step 103 err: 17942.0\n",
      "Step 104 err: 17942.0\n",
      "Step 105 err: 17942.0\n",
      "Step 106 err: 17942.0\n",
      "Step 107 err: 17942.0\n",
      "Step 108 err: 17942.0\n",
      "Step 109 err: 17942.0\n",
      "Step 110 err: 17942.0\n",
      "Step 111 err: 17942.0\n",
      "Step 112 err: 17942.0\n",
      "Step 113 err: 17942.0\n",
      "Step 114 err: 17942.0\n",
      "Step 115 err: 17942.0\n",
      "Step 116 err: 17942.0\n",
      "Step 117 err: 17942.0\n",
      "Step 118 err: 17942.0\n",
      "Step 119 err: 17942.0\n",
      "Step 120 err: 17942.0\n",
      "Step 121 err: 17942.0\n",
      "Step 122 err: 17942.0\n",
      "Step 123 err: 17942.0\n",
      "Step 124 err: 17942.0\n",
      "Step 125 err: 17942.0\n",
      "Step 126 err: 17942.0\n",
      "Step 127 err: 17942.0\n",
      "Step 128 err: 17942.0\n",
      "Step 129 err: 17942.0\n",
      "Step 130 err: 17942.0\n",
      "Step 131 err: 17942.0\n",
      "Step 132 err: 17942.0\n",
      "Step 133 err: 17942.0\n",
      "Step 134 err: 17942.0\n",
      "Step 135 err: 17942.0\n",
      "Step 136 err: 17942.0\n",
      "Step 137 err: 17942.0\n",
      "Step 138 err: 17942.0\n",
      "Step 139 err: 17942.0\n",
      "Step 140 err: 17942.0\n",
      "Step 141 err: 17942.0\n",
      "Step 142 err: 17942.0\n",
      "Step 143 err: 17942.0\n",
      "Step 144 err: 17942.0\n",
      "Step 145 err: 17942.0\n",
      "Step 146 err: 17942.0\n",
      "Step 147 err: 17942.0\n",
      "Step 148 err: 17942.0\n",
      "Step 149 err: 17942.0\n",
      "Step 150 err: 17942.0\n",
      "Step 151 err: 17942.0\n",
      "Step 152 err: 17942.0\n",
      "Step 153 err: 17942.0\n",
      "Step 154 err: 17942.0\n",
      "Step 155 err: 17942.0\n",
      "Step 156 err: 17942.0\n",
      "Step 157 err: 17942.0\n",
      "Step 158 err: 17942.0\n",
      "Step 159 err: 17942.0\n",
      "Step 160 err: 17942.0\n",
      "Step 161 err: 17942.0\n",
      "Step 162 err: 17942.0\n",
      "Step 163 err: 17942.0\n",
      "Step 164 err: 17942.0\n",
      "Step 165 err: 17942.0\n",
      "Step 166 err: 17942.0\n",
      "Step 167 err: 17942.0\n",
      "Step 168 err: 17942.0\n",
      "Step 169 err: 17942.0\n",
      "Step 170 err: 17942.0\n",
      "Step 171 err: 17942.0\n",
      "Step 172 err: 17942.0\n",
      "Step 173 err: 17942.0\n",
      "Step 174 err: 17942.0\n",
      "Step 175 err: 17942.0\n",
      "Step 176 err: 17942.0\n",
      "Step 177 err: 17942.0\n",
      "Step 178 err: 17942.0\n",
      "Step 179 err: 17942.0\n",
      "Step 180 err: 17942.0\n",
      "Step 181 err: 17942.0\n",
      "Step 182 err: 17942.0\n",
      "Step 183 err: 17942.0\n",
      "Step 184 err: 17942.0\n",
      "Step 185 err: 17942.0\n",
      "Step 186 err: 17942.0\n",
      "Step 187 err: 17942.0\n",
      "Step 188 err: 17942.0\n",
      "Step 189 err: 17942.0\n",
      "Step 190 err: 17942.0\n",
      "Step 191 err: 17942.0\n",
      "Step 192 err: 17942.0\n",
      "Step 193 err: 17942.0\n",
      "Step 194 err: 17942.0\n",
      "Step 195 err: 17942.0\n",
      "Step 196 err: 17942.0\n",
      "Step 197 err: 17942.0\n",
      "Step 198 err: 17942.0\n",
      "Step 199 err: 17942.0\n",
      "Step 200 err: 17942.0\n",
      "Step 201 err: 17942.0\n",
      "Step 202 err: 17942.0\n",
      "Step 203 err: 17942.0\n",
      "Step 204 err: 17942.0\n",
      "Step 205 err: 17942.0\n",
      "Step 206 err: 17942.0\n",
      "Step 207 err: 17942.0\n",
      "Step 208 err: 17942.0\n",
      "Step 209 err: 17942.0\n",
      "Step 210 err: 17942.0\n",
      "Step 211 err: 17942.0\n",
      "Step 212 err: 17942.0\n",
      "Step 213 err: 17942.0\n",
      "Step 214 err: 17942.0\n",
      "Step 215 err: 17942.0\n",
      "Step 216 err: 17942.0\n",
      "Step 217 err: 17942.0\n",
      "Step 218 err: 17942.0\n",
      "Step 219 err: 17942.0\n",
      "Step 220 err: 17942.0\n",
      "Step 221 err: 17942.0\n",
      "Step 222 err: 17942.0\n",
      "Step 223 err: 17942.0\n",
      "Step 224 err: 17942.0\n",
      "Step 225 err: 17942.0\n",
      "Step 226 err: 17942.0\n",
      "Step 227 err: 17942.0\n",
      "Step 228 err: 17942.0\n",
      "Step 229 err: 17942.0\n",
      "Step 230 err: 17942.0\n",
      "Step 231 err: 17942.0\n",
      "Step 232 err: 17942.0\n",
      "Step 233 err: 17942.0\n",
      "Step 234 err: 17942.0\n",
      "Step 235 err: 17942.0\n",
      "Step 236 err: 17942.0\n",
      "Step 237 err: 17942.0\n",
      "Step 238 err: 17942.0\n",
      "Step 239 err: 17942.0\n",
      "Step 240 err: 17942.0\n",
      "Step 241 err: 17942.0\n",
      "Step 242 err: 17942.0\n",
      "Step 243 err: 17942.0\n",
      "Step 244 err: 17942.0\n",
      "Step 245 err: 17942.0\n",
      "Step 246 err: 17942.0\n",
      "Step 247 err: 17942.0\n",
      "Step 248 err: 17942.0\n",
      "Step 249 err: 17942.0\n",
      "Step 250 err: 17942.0\n",
      "Step 251 err: 17942.0\n",
      "Step 252 err: 17942.0\n",
      "Step 253 err: 17942.0\n",
      "Step 254 err: 17942.0\n",
      "Step 255 err: 17942.0\n",
      "Step 256 err: 17942.0\n",
      "Step 257 err: 17942.0\n",
      "Step 258 err: 17942.0\n",
      "Step 259 err: 17942.0\n",
      "Step 260 err: 17942.0\n",
      "Step 261 err: 17942.0\n",
      "Step 262 err: 17942.0\n",
      "Step 263 err: 17942.0\n",
      "Step 264 err: 17942.0\n",
      "Step 265 err: 17942.0\n",
      "Step 266 err: 17942.0\n",
      "Step 267 err: 17942.0\n",
      "Step 268 err: 17942.0\n",
      "Step 269 err: 17942.0\n",
      "Step 270 err: 17942.0\n",
      "Step 271 err: 17942.0\n",
      "Step 272 err: 17942.0\n",
      "Step 273 err: 17942.0\n",
      "Step 274 err: 17942.0\n",
      "Step 275 err: 17942.0\n",
      "Step 276 err: 17942.0\n",
      "Step 277 err: 17942.0\n",
      "Step 278 err: 17942.0\n",
      "Step 279 err: 17942.0\n",
      "Step 280 err: 17942.0\n",
      "Step 281 err: 17942.0\n",
      "Step 282 err: 17942.0\n",
      "Step 283 err: 17942.0\n",
      "Step 284 err: 17942.0\n",
      "Step 285 err: 17942.0\n",
      "Step 286 err: 17942.0\n",
      "Step 287 err: 17942.0\n",
      "Step 288 err: 17942.0\n",
      "Step 289 err: 17942.0\n",
      "Step 290 err: 17942.0\n",
      "Step 291 err: 17942.0\n",
      "Step 292 err: 17942.0\n",
      "Step 293 err: 17942.0\n",
      "Step 294 err: 17942.0\n",
      "Step 295 err: 17942.0\n",
      "Step 296 err: 17942.0\n",
      "Step 297 err: 17942.0\n",
      "Step 298 err: 17942.0\n",
      "Step 299 err: 17942.0\n",
      "Step 300 err: 17942.0\n",
      "Step 301 err: 17942.0\n",
      "Step 302 err: 17942.0\n",
      "Step 303 err: 17942.0\n",
      "Step 304 err: 17942.0\n",
      "Step 305 err: 17942.0\n",
      "Step 306 err: 17942.0\n",
      "Step 307 err: 17942.0\n",
      "Step 308 err: 17942.0\n",
      "Step 309 err: 17942.0\n",
      "Step 310 err: 17942.0\n",
      "Step 311 err: 17942.0\n",
      "Step 312 err: 17942.0\n",
      "Step 313 err: 17942.0\n",
      "Step 314 err: 17942.0\n",
      "Step 315 err: 17942.0\n",
      "Step 316 err: 17942.0\n",
      "Step 317 err: 17942.0\n",
      "Step 318 err: 17942.0\n",
      "Step 319 err: 17942.0\n",
      "Step 320 err: 17942.0\n",
      "Step 321 err: 17942.0\n",
      "Step 322 err: 17942.0\n",
      "Step 323 err: 17942.0\n",
      "Step 324 err: 17942.0\n",
      "Step 325 err: 17942.0\n",
      "Step 326 err: 17942.0\n",
      "Step 327 err: 17942.0\n",
      "Step 328 err: 17942.0\n",
      "Step 329 err: 17942.0\n",
      "Step 330 err: 17942.0\n",
      "Step 331 err: 17942.0\n",
      "Step 332 err: 17942.0\n",
      "Step 333 err: 17942.0\n",
      "Step 334 err: 17942.0\n",
      "Step 335 err: 17942.0\n",
      "Step 336 err: 17942.0\n",
      "Step 337 err: 17942.0\n",
      "Step 338 err: 17942.0\n",
      "Step 339 err: 17942.0\n",
      "Step 340 err: 17942.0\n",
      "Step 341 err: 17942.0\n",
      "Step 342 err: 17942.0\n",
      "Step 343 err: 17942.0\n",
      "Step 344 err: 17942.0\n",
      "Step 345 err: 17942.0\n"
     ]
    }
   ],
   "source": [
    "fea_dist_method = 'Euclidean'\n",
    "image_dist_method = 'Euclidean'\n",
    "error = 'abs'\n",
    "result_dir = '/content/ResultsTest_1'\n",
    "os.makedirs(name=result_dir, exist_ok=True)\n",
    "euclidean_data, euclidean_samples = table_to_image(features_scaled, [num_row, num_col], fea_dist_method, image_dist_method, save_image_size,\n",
    "               max_step, val_step, result_dir, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKPNjpe2n8al"
   },
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DekMQB2hk5tA",
    "outputId": "f5270781-81ea-4eb0-8d04-a30226e066f3"
   },
   "outputs": [],
   "source": [
    "# Reshape the features to (336, 3*3)\n",
    "features = copy.deepcopy(euclidean_data)\n",
    "reshaped_features = features.reshape(-1, num_row*num_col)\n",
    "\n",
    "# Split the data and labels using train_test_split\n",
    "train_data, valid_data, train_labels, valid_labels = train_test_split(\n",
    "    reshaped_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the split data back to the original shape\n",
    "train_data = train_data.reshape(-1, num_row, num_col)\n",
    "valid_data = valid_data.reshape(-1, num_row, num_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C71pV66bsGeN",
    "outputId": "c0afd84b-0f82-4bbc-aefb-44556f1f0272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50861/50861 [==============================] - 148s 3ms/step - loss: 1864.2917 - mae: 25.5949 - val_loss: 63.7377 - val_mae: 5.8580\n",
      "Epoch 2/20\n",
      "50861/50861 [==============================] - 155s 3ms/step - loss: 400.5799 - mae: 14.3034 - val_loss: 35.9982 - val_mae: 4.2561\n",
      "Epoch 3/20\n",
      "50861/50861 [==============================] - 155s 3ms/step - loss: 305.4468 - mae: 12.5615 - val_loss: 40.8064 - val_mae: 5.0479\n",
      "Epoch 4/20\n",
      "50861/50861 [==============================] - 155s 3ms/step - loss: 268.1950 - mae: 11.8131 - val_loss: 53.2451 - val_mae: 5.4178\n",
      "Epoch 5/20\n",
      "50861/50861 [==============================] - 160s 3ms/step - loss: 244.4930 - mae: 11.2613 - val_loss: 35.2404 - val_mae: 4.4076\n",
      "Epoch 6/20\n",
      "50861/50861 [==============================] - 168s 3ms/step - loss: 221.4330 - mae: 10.6821 - val_loss: 35.4156 - val_mae: 4.1513\n",
      "Epoch 7/20\n",
      "50861/50861 [==============================] - 169s 3ms/step - loss: 197.3090 - mae: 10.0262 - val_loss: 30.8834 - val_mae: 4.0201\n",
      "Epoch 8/20\n",
      "50861/50861 [==============================] - 168s 3ms/step - loss: 179.8053 - mae: 9.4698 - val_loss: 33.6565 - val_mae: 4.2398\n",
      "Epoch 9/20\n",
      "50861/50861 [==============================] - 172s 3ms/step - loss: 171.3882 - mae: 9.1590 - val_loss: 25.4075 - val_mae: 3.4405\n",
      "Epoch 10/20\n",
      "50861/50861 [==============================] - 166s 3ms/step - loss: 167.3705 - mae: 8.9847 - val_loss: 22.3464 - val_mae: 3.4339\n",
      "Epoch 11/20\n",
      "50861/50861 [==============================] - 167s 3ms/step - loss: 164.7238 - mae: 8.8689 - val_loss: 35.2882 - val_mae: 4.4541\n",
      "Epoch 12/20\n",
      "50861/50861 [==============================] - 175s 3ms/step - loss: 162.6559 - mae: 8.7904 - val_loss: 27.2328 - val_mae: 3.9246\n",
      "Epoch 13/20\n",
      "50861/50861 [==============================] - 175s 3ms/step - loss: 161.3981 - mae: 8.7460 - val_loss: 19.5735 - val_mae: 3.4668\n",
      "Epoch 14/20\n",
      "50861/50861 [==============================] - 175s 3ms/step - loss: 161.6107 - mae: 8.7276 - val_loss: 29.8874 - val_mae: 3.9387\n",
      "Epoch 15/20\n",
      "50861/50861 [==============================] - 163s 3ms/step - loss: 161.8769 - mae: 8.7190 - val_loss: 30.9245 - val_mae: 4.1105\n",
      "Epoch 16/20\n",
      "50861/50861 [==============================] - 148s 3ms/step - loss: 161.2377 - mae: 8.6892 - val_loss: 24.7548 - val_mae: 3.7328\n",
      "Epoch 17/20\n",
      "50861/50861 [==============================] - 169s 3ms/step - loss: 161.1169 - mae: 8.6684 - val_loss: 21.6501 - val_mae: 3.4163\n",
      "Epoch 18/20\n",
      "50861/50861 [==============================] - 180s 4ms/step - loss: 159.8540 - mae: 8.6370 - val_loss: 15.9422 - val_mae: 3.0394\n",
      "Epoch 19/20\n",
      "50861/50861 [==============================] - 180s 4ms/step - loss: 160.7373 - mae: 8.6420 - val_loss: 24.0317 - val_mae: 3.5581\n",
      "Epoch 20/20\n",
      "50861/50861 [==============================] - 157s 3ms/step - loss: 160.5256 - mae: 8.6182 - val_loss: 48.2290 - val_mae: 5.3330\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define encoder\n",
    "n_inputs = 5\n",
    "visible = Input(shape=(5, 5))\n",
    "\n",
    "# encoder level 1\n",
    "e = Dense(n_inputs*2)(visible)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "\n",
    "# encoder level 2\n",
    "e = Dense(n_inputs)(e)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "\n",
    "# bottleneck\n",
    "n_bottleneck = n_inputs\n",
    "bottleneck = Dense(n_bottleneck)(e)\n",
    "\n",
    "# define decoder, level 1\n",
    "d = Dense(n_inputs)(bottleneck)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "\n",
    "# decoder level 2\n",
    "d = Dense(n_inputs*2)(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "\n",
    "# output layer\n",
    "output = Dense(n_inputs, activation='linear')(d)\n",
    "\n",
    "# define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[\"mae\"])\n",
    "\n",
    "# fit the autoencoder model to reconstruct input\n",
    "history = model.fit(train_data, train_data, epochs=20, batch_size=16, validation_data=(valid_data, valid_data))\n",
    "\n",
    "# plot loss\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='validation')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "a-A0OJT2cxG6"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
